<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>nimbletl.tasks API documentation</title>
<meta name="description" content="Module with prefects utility functions and tasks â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nimbletl.tasks</code></h1>
</header>
<section id="section-intro">
<p>Module with prefects utility functions and tasks.</p>
<p>Use <code>prefect.task</code> to a function into a task in your dataflow pipeline, for example:</p>
<div class="admonition code">
<p class="admonition-title">Code:&ensp;python</p>
<p>unzip_task = task(unzip)</p>
</div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Module with prefects utility functions and tasks.

Use `prefect.task` to a function into a task in your dataflow pipeline, for example:

.. code:: python

    unzip_task = task(unzip)

&#34;&#34;&#34;

import datetime
from pathlib import Path
import requests
from typing import Union, Any
from zipfile import ZipFile

from google.cloud import bigquery
import pandas as pd
import prefect
from prefect import task
from prefect.utilities.tasks import defaults_from_attrs
from prefect.tasks.gcp.bigquery import BigQueryLoadFile
from prefect.engine.signals import SKIP
from prefect.tasks.shell import ShellTask
from prefect.tasks.templates import StringFormatter

from nimbletl.utilities import clean_python_name


@task
def curl_cmd(url: str, filepath: Union[str, Path], **kwargs) -&gt; str:
    &#34;&#34;&#34;Template for curl command to download file.

    Uses `curl -fL -o` that fails silently and follows redirects. 

    Example:
    ```
    from pathlib import Path

    from prefect import Parameter, Flow
    from prefect.tasks.shell import ShellTask

    curl_download = ShellTask(name=&#39;curl_download&#39;)

    with Flow(&#39;test&#39;) as flow:
        filepath = Parameter(&#34;filepath&#34;, required=True)
        curl_command = curl_cmd(&#34;https://some/url&#34;, filepath)
        curl_download = curl_download(command=curl_command)

    flow.run(parameters={&#39;filepath&#39;: Path.home() / &#39;test.zip&#39;})
    ```

    Args:
        - url (str): url to download
        - file (str): file for saving fecthed url
        - **kwargs: passed to Task constructor
    
    Returns:
        str: curl command
    
    Raises:
        - SKIP: if filepath exists
    &#34;&#34;&#34;
    if Path(filepath).exists():
        raise SKIP(f&#34;File {filepath} already exists.&#34;)
    return f&#34;curl -fL -o {filepath} {url}&#34;


def excel_to_gbq(io=None, destination=None, credentials=None, GCP=None):
    &#34;&#34;&#34;Load Excel to BigQuery.

    Args:
        - io: str, bytes, ExcelFile, xlrd.Book, path object, or file-like object passed to `pandas.read_excel`
        - destination_table (str): name of destination table in BigQuery in format `dataset.tablename`      
        - credentials (google.auth.credentials.Credentials): credentials for project and BigQuery
        - GCP (dataclass): configuration object with `project` and `location` attributes
    
    Returns:
        - google.cloud.bigquery.job.LoadJob
    &#34;&#34;&#34;
    df = pd.read_excel(io).rename(columns=clean_python_name)
    bq = bigquery.Client(credentials=credentials, project=GCP.project)
    job_config = bigquery.LoadJobConfig()
    job_config.write_disposition= &#34;WRITE_TRUNCATE&#34;
    job = bq.load_table_from_dataframe(
        dataframe=df,
        destination=destination_table,
        job_config=job_config,
        credentials=credentials,
        project=GCP.project,
        location=GCP.location,
    )
    return job


def unzip(zipfile):
    &#34;&#34;&#34;Extracts zipfile from path in the same directory.

    Replaces original zipfile with empty file, so downstream tasks know the file is there.

    Args:
        - path: Path-object to zipfile
        - zipfile

    Returns:
        Path-objects of extracted files
    &#34;&#34;&#34;
    with ZipFile(zipfile) as zip:
        zip.extractall(path=zipfile.parent)
        files = [zipfile.parent / f for f in zip.namelist()]

    zipfile.unlink()
    zipfile.touch()
    return files


def create_dir(path: Path) -&gt; Path:
    &#34;&#34;&#34;Checks whether path exists and is directory, and creates it if not.
    
    Args:
        - path (Path): path to check
    
    Returns:
        - Path: new directory
    &#34;&#34;&#34;
    try:
        path = Path(path)
        if not (path.exists() and path.is_dir()):
            path.mkdir(parents=True)
        return path
    except TypeError as error:
        print(f&#34;Error trying to find {path}: {error!s}&#34;)
        return None

@task
def cbsodatav3_to_gbq(id, third_party=False, schema=&#34;cbs&#34;, credentials=None, GCP=None):
    &#34;&#34;&#34;Load CBS odata v3 into Google BigQuery.

    For given dataset id, following tables are uploaded into schema (taking `cbs` as default and `83583NED` as example):

    - cbs.83583NED_DataProperties: description of topics and dimensions contained in table
    - cbs.83583NED_DimensionName: separate dimension tables
    - cbs.83583NED_TypedDataSet: the TypedDataset
    - cbs.83583NED_CategoryGroups: grouping of dimensions

    See Handleiding CBS Open Data Services (v3)[^odatav3] for details.
    
    Args:
        - id (str): table ID like `83583NED`
        - third_party (boolean): &#39;opendata.cbs.nl&#39; is used by default (False). Set to true for dataderden.cbs.nl
        - schema (str): schema to load data into
        - credentials: GCP credentials
        - GCP: config object

    Return:
        - List[google.cloud.bigquery.job.LoadJob] 

    [^odatav3]: https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-opendata-services.pdf
    &#34;&#34;&#34;
    
    base_url = {
        True: f&#34;https://dataderden.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
        False: f&#34;https://opendata.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
    }
    urls = {
        item[&#34;name&#34;]: item[&#34;url&#34;]
    for item in requests.get(base_url[third_party]).json()[&#34;value&#34;]
    }
    print(urls)

    bq = bigquery.Client(project=GCP.project)
    job_config = bigquery.LoadJobConfig()

    # Need to append because API may return more than 1 rowset (max 10.000 rows per call)
    job_config.write_disposition = &#34;WRITE_APPEND&#34;
    jobs = []

    # TableInfos is redundant --&gt; use https://opendata.cbs.nl/ODataCatalog/Tables?$format=json
    # UntypedDataSet is redundant --&gt; use TypedDataSet
    for key, url in [
        (k, v) for k, v in urls.items() if k not in (&#34;TableInfos&#34;, &#34;UntypedDataSet&#34;)
    ]:
        url = &#34;?&#34;.join((url, &#34;$format=json&#34;))
        table_name = f&#34;{schema}.{id}_{key}&#34;
        bq.delete_table(table=table_name, not_found_ok=True)

        i = 0
        while url:
            logger = prefect.context.get(&#34;logger&#34;)
            logger.info(f&#34;Processing {key} (i = {i}) from {url}&#34;)
            r = requests.get(url).json()

            # odata api contains empty lists as values --&gt; skip these
            if r[&#34;value&#34;]:
                # DataProperties contains column odata.type --&gt; odata_type
                df = pd.DataFrame(r[&#34;value&#34;]).rename(
                    columns=lambda s: s.replace(&#34;.&#34;, &#34;_&#34;)
                )
                jobs.append(
                    bq.load_table_from_dataframe(
                        df,
                        destination=table_name,
                        project=GCP.project,
                        job_config=job_config,
                    )
                )

            # each request limited to 10,000 cells
            if &#34;odata.nextLink&#34; in r:
                i += 1
                url = r[&#34;odata.nextLink&#34;]
            else:
                url = None
    return jobs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="nimbletl.tasks.cbsodatav3_to_gbq"><code class="name flex">
<span>def <span class="ident">cbsodatav3_to_gbq</span></span>(<span>id, third_party=False, schema='cbs', credentials=None, GCP=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Load CBS odata v3 into Google BigQuery.</p>
<p>For given dataset id, following tables are uploaded into schema (taking <code>cbs</code> as default and <code>83583NED</code> as example):</p>
<ul>
<li>cbs.83583NED_DataProperties: description of topics and dimensions contained in table</li>
<li>cbs.83583NED_DimensionName: separate dimension tables</li>
<li>cbs.83583NED_TypedDataSet: the TypedDataset</li>
<li>cbs.83583NED_CategoryGroups: grouping of dimensions</li>
</ul>
<p>See Handleiding CBS Open Data Services (v3)<sup id="fnref:odatav3"><a class="footnote-ref" href="#fn:odatav3">1</a></sup> for details.</p>
<h2 id="args">Args</h2>
<ul>
<li>id (str): table ID like <code>83583NED</code></li>
<li>third_party (boolean): 'opendata.cbs.nl' is used by default (False). Set to true for dataderden.cbs.nl</li>
<li>schema (str): schema to load data into</li>
<li>credentials: GCP credentials</li>
<li>GCP: config object</li>
</ul>
<h2 id="return">Return</h2>
<ul>
<li>List[google.cloud.bigquery.job.LoadJob] </li>
</ul>
<div class="footnote">
<hr>
<ol>
<li id="fn:odatav3">
<p><a href="https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-opendata-services.pdf">https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-opendata-services.pdf</a>&#160;<a class="footnote-backref" href="#fnref:odatav3" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def cbsodatav3_to_gbq(id, third_party=False, schema=&#34;cbs&#34;, credentials=None, GCP=None):
    &#34;&#34;&#34;Load CBS odata v3 into Google BigQuery.

    For given dataset id, following tables are uploaded into schema (taking `cbs` as default and `83583NED` as example):

    - cbs.83583NED_DataProperties: description of topics and dimensions contained in table
    - cbs.83583NED_DimensionName: separate dimension tables
    - cbs.83583NED_TypedDataSet: the TypedDataset
    - cbs.83583NED_CategoryGroups: grouping of dimensions

    See Handleiding CBS Open Data Services (v3)[^odatav3] for details.
    
    Args:
        - id (str): table ID like `83583NED`
        - third_party (boolean): &#39;opendata.cbs.nl&#39; is used by default (False). Set to true for dataderden.cbs.nl
        - schema (str): schema to load data into
        - credentials: GCP credentials
        - GCP: config object

    Return:
        - List[google.cloud.bigquery.job.LoadJob] 

    [^odatav3]: https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-opendata-services.pdf
    &#34;&#34;&#34;
    
    base_url = {
        True: f&#34;https://dataderden.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
        False: f&#34;https://opendata.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
    }
    urls = {
        item[&#34;name&#34;]: item[&#34;url&#34;]
    for item in requests.get(base_url[third_party]).json()[&#34;value&#34;]
    }
    print(urls)

    bq = bigquery.Client(project=GCP.project)
    job_config = bigquery.LoadJobConfig()

    # Need to append because API may return more than 1 rowset (max 10.000 rows per call)
    job_config.write_disposition = &#34;WRITE_APPEND&#34;
    jobs = []

    # TableInfos is redundant --&gt; use https://opendata.cbs.nl/ODataCatalog/Tables?$format=json
    # UntypedDataSet is redundant --&gt; use TypedDataSet
    for key, url in [
        (k, v) for k, v in urls.items() if k not in (&#34;TableInfos&#34;, &#34;UntypedDataSet&#34;)
    ]:
        url = &#34;?&#34;.join((url, &#34;$format=json&#34;))
        table_name = f&#34;{schema}.{id}_{key}&#34;
        bq.delete_table(table=table_name, not_found_ok=True)

        i = 0
        while url:
            logger = prefect.context.get(&#34;logger&#34;)
            logger.info(f&#34;Processing {key} (i = {i}) from {url}&#34;)
            r = requests.get(url).json()

            # odata api contains empty lists as values --&gt; skip these
            if r[&#34;value&#34;]:
                # DataProperties contains column odata.type --&gt; odata_type
                df = pd.DataFrame(r[&#34;value&#34;]).rename(
                    columns=lambda s: s.replace(&#34;.&#34;, &#34;_&#34;)
                )
                jobs.append(
                    bq.load_table_from_dataframe(
                        df,
                        destination=table_name,
                        project=GCP.project,
                        job_config=job_config,
                    )
                )

            # each request limited to 10,000 cells
            if &#34;odata.nextLink&#34; in r:
                i += 1
                url = r[&#34;odata.nextLink&#34;]
            else:
                url = None
    return jobs</code></pre>
</details>
</dd>
<dt id="nimbletl.tasks.create_dir"><code class="name flex">
<span>def <span class="ident">create_dir</span></span>(<span>path:Â pathlib.Path) â€‘>Â pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether path exists and is directory, and creates it if not.</p>
<h2 id="args">Args</h2>
<ul>
<li>path (Path): path to check</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>Path: new directory</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_dir(path: Path) -&gt; Path:
    &#34;&#34;&#34;Checks whether path exists and is directory, and creates it if not.
    
    Args:
        - path (Path): path to check
    
    Returns:
        - Path: new directory
    &#34;&#34;&#34;
    try:
        path = Path(path)
        if not (path.exists() and path.is_dir()):
            path.mkdir(parents=True)
        return path
    except TypeError as error:
        print(f&#34;Error trying to find {path}: {error!s}&#34;)
        return None</code></pre>
</details>
</dd>
<dt id="nimbletl.tasks.curl_cmd"><code class="name flex">
<span>def <span class="ident">curl_cmd</span></span>(<span>url:Â str, filepath:Â Union[str,Â pathlib.Path], **kwargs) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Template for curl command to download file.</p>
<p>Uses <code>curl -fL -o</code> that fails silently and follows redirects. </p>
<p>Example:</p>
<pre><code>from pathlib import Path

from prefect import Parameter, Flow
from prefect.tasks.shell import ShellTask

curl_download = ShellTask(name='curl_download')

with Flow('test') as flow:
    filepath = Parameter(&quot;filepath&quot;, required=True)
    curl_command = curl_cmd(&quot;https://some/url&quot;, filepath)
    curl_download = curl_download(command=curl_command)

flow.run(parameters={'filepath': Path.home() / 'test.zip'})
</code></pre>
<h2 id="args">Args</h2>
<ul>
<li>url (str): url to download</li>
<li>file (str): file for saving fecthed url</li>
<li>**kwargs: passed to Task constructor</li>
</ul>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>curl command</dd>
</dl>
<h2 id="raises">Raises</h2>
<ul>
<li>SKIP: if filepath exists</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def curl_cmd(url: str, filepath: Union[str, Path], **kwargs) -&gt; str:
    &#34;&#34;&#34;Template for curl command to download file.

    Uses `curl -fL -o` that fails silently and follows redirects. 

    Example:
    ```
    from pathlib import Path

    from prefect import Parameter, Flow
    from prefect.tasks.shell import ShellTask

    curl_download = ShellTask(name=&#39;curl_download&#39;)

    with Flow(&#39;test&#39;) as flow:
        filepath = Parameter(&#34;filepath&#34;, required=True)
        curl_command = curl_cmd(&#34;https://some/url&#34;, filepath)
        curl_download = curl_download(command=curl_command)

    flow.run(parameters={&#39;filepath&#39;: Path.home() / &#39;test.zip&#39;})
    ```

    Args:
        - url (str): url to download
        - file (str): file for saving fecthed url
        - **kwargs: passed to Task constructor
    
    Returns:
        str: curl command
    
    Raises:
        - SKIP: if filepath exists
    &#34;&#34;&#34;
    if Path(filepath).exists():
        raise SKIP(f&#34;File {filepath} already exists.&#34;)
    return f&#34;curl -fL -o {filepath} {url}&#34;</code></pre>
</details>
</dd>
<dt id="nimbletl.tasks.excel_to_gbq"><code class="name flex">
<span>def <span class="ident">excel_to_gbq</span></span>(<span>io=None, destination=None, credentials=None, GCP=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Load Excel to BigQuery.</p>
<h2 id="args">Args</h2>
<ul>
<li>io: str, bytes, ExcelFile, xlrd.Book, path object, or file-like object passed to <code>pandas.read_excel</code></li>
<li>destination_table (str): name of destination table in BigQuery in format <code>dataset.tablename</code>
</li>
<li>credentials (google.auth.credentials.Credentials): credentials for project and BigQuery</li>
<li>GCP (dataclass): configuration object with <code>project</code> and <code>location</code> attributes</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>google.cloud.bigquery.job.LoadJob</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def excel_to_gbq(io=None, destination=None, credentials=None, GCP=None):
    &#34;&#34;&#34;Load Excel to BigQuery.

    Args:
        - io: str, bytes, ExcelFile, xlrd.Book, path object, or file-like object passed to `pandas.read_excel`
        - destination_table (str): name of destination table in BigQuery in format `dataset.tablename`      
        - credentials (google.auth.credentials.Credentials): credentials for project and BigQuery
        - GCP (dataclass): configuration object with `project` and `location` attributes
    
    Returns:
        - google.cloud.bigquery.job.LoadJob
    &#34;&#34;&#34;
    df = pd.read_excel(io).rename(columns=clean_python_name)
    bq = bigquery.Client(credentials=credentials, project=GCP.project)
    job_config = bigquery.LoadJobConfig()
    job_config.write_disposition= &#34;WRITE_TRUNCATE&#34;
    job = bq.load_table_from_dataframe(
        dataframe=df,
        destination=destination_table,
        job_config=job_config,
        credentials=credentials,
        project=GCP.project,
        location=GCP.location,
    )
    return job</code></pre>
</details>
</dd>
<dt id="nimbletl.tasks.unzip"><code class="name flex">
<span>def <span class="ident">unzip</span></span>(<span>zipfile)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts zipfile from path in the same directory.</p>
<p>Replaces original zipfile with empty file, so downstream tasks know the file is there.</p>
<h2 id="args">Args</h2>
<ul>
<li>path: Path-object to zipfile</li>
<li>zipfile</li>
</ul>
<h2 id="returns">Returns</h2>
<p>Path-objects of extracted files</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unzip(zipfile):
    &#34;&#34;&#34;Extracts zipfile from path in the same directory.

    Replaces original zipfile with empty file, so downstream tasks know the file is there.

    Args:
        - path: Path-object to zipfile
        - zipfile

    Returns:
        Path-objects of extracted files
    &#34;&#34;&#34;
    with ZipFile(zipfile) as zip:
        zip.extractall(path=zipfile.parent)
        files = [zipfile.parent / f for f in zip.namelist()]

    zipfile.unlink()
    zipfile.touch()
    return files</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nimbletl" href="index.html">nimbletl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="nimbletl.tasks.cbsodatav3_to_gbq" href="#nimbletl.tasks.cbsodatav3_to_gbq">cbsodatav3_to_gbq</a></code></li>
<li><code><a title="nimbletl.tasks.create_dir" href="#nimbletl.tasks.create_dir">create_dir</a></code></li>
<li><code><a title="nimbletl.tasks.curl_cmd" href="#nimbletl.tasks.curl_cmd">curl_cmd</a></code></li>
<li><code><a title="nimbletl.tasks.excel_to_gbq" href="#nimbletl.tasks.excel_to_gbq">excel_to_gbq</a></code></li>
<li><code><a title="nimbletl.tasks.unzip" href="#nimbletl.tasks.unzip">unzip</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>